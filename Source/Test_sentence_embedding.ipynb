{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototype_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nn9QH03IvR6B"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FoLvs25FSEg"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okXB7xudFaMJ"
      },
      "source": [
        "import plotly.offline as pyoff\n",
        "import plotly.graph_objs as go"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJY33P3cFavd"
      },
      "source": [
        "import plotly.graph_objs as go"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouFruo9fFo00"
      },
      "source": [
        "fake = pd.read_csv('./Fake.csv')\n",
        "real = pd.read_csv('./Real.csv')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAZvKkhlFsbN"
      },
      "source": [
        "fake['Target']=1\n",
        "real['Target']=0"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jB7hbDlF0dh"
      },
      "source": [
        "frames = [fake, real]\n",
        "\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1y17jXAF1mW",
        "outputId": "d2568845-c1fc-45eb-ea2f-9f737f74ab3b"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44898"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAxa5A3APV0n"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_=df.copy()\n",
        "df_=df_.sort_values(by=['date'])\n",
        "df_=df_.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "V6vVKk-cpliu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_['news']=df_['subject']+' '+df_['title']+' '+df_['text']\n",
        "df_['news'] = df_.apply(lambda x: x['news'].lower(),axis=1)\n",
        "df_[\"news\"] = df_['news'].str.replace('[^\\w\\s]','')\n",
        "all_news=pd.DataFrame(pd.Series(' '.join(df_['news']).split()).value_counts())\n",
        "allnews1=all_news.head(30)\n"
      ],
      "metadata": {
        "id": "y8LrwT3upp-U"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6NAlNL6pwbd",
        "outputId": "08f2f401-c526-4258-b787-fbf91ed54d18"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_clean(text):\n",
        "  \"\"\"\n",
        "  A simple function to clean up the data. All the words that\n",
        "  are not designated as a stop word is then lemmatized after\n",
        "  encoding and basic regex parsing are performed.\n",
        "  \"\"\"\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]"
      ],
      "metadata": {
        "id": "DG4UqdExo33k"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = basic_clean(''.join(str(df_['news'].tolist())))\n"
      ],
      "metadata": {
        "id": "txfpbdYupOZk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Embedding**"
      ],
      "metadata": {
        "id": "247aosErvkHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df_['Target'].values\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "4t7OqjV_vqj5"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "metadata": {
        "id": "Bh-cVTxazbko"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df_, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "7hlqo8mPAt9u"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged = train.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['news']), tags=[r.Target]), axis=1)\n",
        "test_tagged = test.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['news']), tags=[r.Target]), axis=1)"
      ],
      "metadata": {
        "id": "G2H8uP2tAsw3"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged.values[30]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lva3UrpbKugl",
        "outputId": "55fa63fb-85b0-4cec-b1d2-a7d138a2ff03"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=['news', 'the', 'united', 'states', 'government', 'has', 'told', 'trump', 'to', 'go', 'fck', 'himself', 'when', 'trump', 'began', 'his', 'disinformation', 'campaign', 'and', 'then', 'started', 'trying', 'to', 'muzzle', 'federal', 'agencies', 'employees', 'of', 'some', 'of', 'these', 'agencies', 'went', 'to', 'twitter', 'to', 'openly', 'defy', 'him', 'they', 'call', 'themselves', 'altgov', 'and', 'there', 'are', 'dozens', 'of', 'these', 'accounts', 'that', 'are', 'making', 'point', 'of', 'pushing', 'out', 'the', 'very', 'facts', 'that', 'trump', 'wishes', 'would', 'disappear', 'as', 'well', 'as', 'openly', 'fighting', 'his', 'alternative', 'facts', 'and', 'overall', 'disinformation', 'campaign', 'many', 'have', 'been', 'verified', 'by', 'snopes', 'who', 'now', 'keeps', 'running', 'list', 'of', 'trusted', 'altgov', 'accountshowever', 'no', 'matter', 'who', 'they', 'are', 'they', 'are', 'all', 'very', 'clear', 'about', 'the', 'fact', 'that', 'they', 'are', 'not', 'doing', 'this', 'on', 'behalf', 'of', 'their', 'agencies', 'they', 're', 'operating', 'these', 'accounts', 'independently', 'and', 'are', 'not', 'official', 'us', 'government', 'twitter', 'accounts', 'and', 'they', 're', 'dedicated', 'to', 'maintaining', 'the', 'flow', 'of', 'information', 'from', 'the', 'government', 'to', 'the', 'people', 'that', 'we', 'enjoyed', 'under', 'previous', 'administrations', 'please', 'note', 'their', 'work', 'does', 'not', 'appear', 'to', 'include', 'any', 'sensitive', 'information', 'and', 'they', 'appear', 'to', 'be', 'very', 'careful', 'to', 'keep', 'it', 'that', 'waybelow', 'are', 'some', 'of', 'the', 'agencies', 'and', 'departments', 'where', 'members', 'of', 'altgov', 'work', 'have', 'inside', 'knowledge', 'of', 'or', 'can', 'otherwise', 'speak', 'the', 'truth', 'about', 'what', 'the', 'government', 'is', 'doingthere', 'are', 'also', 'two', 'alternate', 'accounts', 'dedicated', 'to', 'sending', 'out', 'facts', 'under', 'the', 'name', 'alt', 'sarah', 'sanders', 'altuspresssec', 'and', 'alt', 'alt', 'press', 'sec', 'altaltpresssec', 'in', 'addition', 'to', 'directly', 'and', 'loudly', 'calling', 'out', 'the', 'trump', 'administration', 'on', 'its', 'lies', 'alt', 'alt', 'press', 'sec', 'speaks', 'truth', 'to', 'power', 'for', 'those', 'who', 'block', 'altuspresssec', 'who', 'has', 'been', 'blocked', 'by', 'none', 'other', 'than', 'trump', 'himself', 'back', 'when', 'sean', 'spicer', 'was', 'still', 'trump', 'press', 'secretary', 'altuspresssec', 'tweeted', 'out', 'short', 'description', 'of', 'what', 'altgov', 'stands', 'forwe', 'are', 'not', 'scientists', 'we', 'are', 'civil', 'servants', 'and', 'federal', 'contractors', 'united', 'for', 'the', 'preservation', 'of', 'democratic', 'norms', 'we', 'support', 'our', 'altgov', 'alt', 'sean', 'spicer', 'altuspresssec', 'january', '26', '2017this', 'is', 'an', 'antitrump', 'resistance', 'movement', 'within', 'the', 'united', 'states', 'government', 'that', 'caught', 'on', 'like', 'wildfire', 'in', 'forest', 'canopy', 'shortly', 'after', 'he', 'decided', 'to', 'muzzle', 'the', 'department', 'of', 'the', 'interior', 'and', 'the', 'national', 'park', 'service', 'back', 'in', 'january', 'there', 'are', 'also', 'many', 'many', 'supporting', 'accounts', 'who', 'aren', 'government', 'but', 'who', 'support', 'the', 'movement', 'and', 'have', 'ways', 'to', 'help', 'what', 'we', 'see', 'here', 'is', 'the', 'backbone', 'of', 'the', 'federal', 'government', 'along', 'with', 'concerned', 'contractors', 'and', 'friends', 'telling', 'trump', 'to', 'fuck', 'right', 'the', 'hell', 'off', 'with', 'his', 'alternative', 'facts', 'and', 'muzzling', 'bullshit', 'they', 'will', 'not', 'be', 'silencedfeatured', 'image', 'by', 'andrew', 'harrer', 'via', 'getty', 'images'], tags=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "sRj2UZHZM4RO"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n"
      ],
      "metadata": {
        "id": "JuoJxJvCM7y9"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow.build_vocab([x for x in train_tagged.values])"
      ],
      "metadata": {
        "id": "8LhgHxHCNUFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "metadata": {
        "id": "MSRL7-0lOL4G"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n"
      ],
      "metadata": {
        "id": "a24ZY-mtOPkm"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "metadata": {
        "id": "wFR243bYSX5h"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9O8VLOuOe8g",
        "outputId": "4b5e5b41-a5f1-4c33-fb17-fc01eb448e28"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy 0.502746844840386\n",
            "Testing F1 score: 0.4857423751572272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Emebedding**"
      ],
      "metadata": {
        "id": "nn9QH03IvR6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_['Target'].values\n",
        "X = []\n",
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df_[\"news\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n",
        "        tmp.extend(filtered_words)\n",
        "    X.append(tmp)"
      ],
      "metadata": {
        "id": "7aOO8kIlrV1r"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)"
      ],
      "metadata": {
        "id": "6w7qUnZYsk6l"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df_, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "_pf-mvU4UFqo"
      },
      "execution_count": 147,
      "outputs": []
    }
  ]
}